{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c4dfcea-51d1-4df4-ba33-d70c77b3c020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "\n",
    "headers_websites = {\"Referer\":None,\n",
    "                    \"Sec-Ch-Ua\":\"'\\\"Not_A Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"120\\\", \\\"Google Chrome\\\";v=\\\"120\\\"'\",\n",
    "                    \"Sec-Ch-Ua-Mobile\":\"?0\",\n",
    "                    \"Sec-Ch-Ua-Platform\": \"Windows\",\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "                   }\n",
    "\n",
    "\n",
    "class main_email_scrapper:\n",
    "    \n",
    "    def __init__(self, original_url):\n",
    "        self.original_url = original_url\n",
    "        \n",
    "        if self.original_url.upper().find('WWW.') < 0:\n",
    "            self.original_url = self.original_url.replace('//','//www.') \n",
    "            \n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.end_time =  self.start_time + datetime.timedelta(seconds=5)\n",
    "        # to save scraped urls\n",
    "        self.scraped = set()\n",
    "        self.steps = 0\n",
    "        # to save fetched emails\n",
    "        self.emails = set() \n",
    "        \n",
    "        self.scrap_emails_from_url(original_url)\n",
    "        \n",
    "        \n",
    "    def finished(self):\n",
    "        return list(self.emails)\n",
    "          \n",
    "    def scrap_emails_from_url(self, url):\n",
    "        if url.find('///') < 0 and url.find('tel:') < 0:\n",
    "            if self.steps < 2:\n",
    "                if datetime.datetime.now() >= self.end_time and len(list(self.emails)) < 1:\n",
    "                    self.end_time =  self.end_time + datetime.timedelta(seconds=5)\n",
    "                    self.steps += 1\n",
    "                if datetime.datetime.now() <= self.end_time:\n",
    "\n",
    "\n",
    "                    if url not in self.scraped:\n",
    "                        self.scraped.add(url)\n",
    "\n",
    "                        parts = urlsplit(url)\n",
    "\n",
    "                        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "\n",
    "                        if '/' in parts.path:\n",
    "                            path = url[:url.rfind('/')+1]\n",
    "                        else:\n",
    "                            path = url\n",
    "\n",
    "                        print(path)\n",
    "\n",
    "                        print(\"Crawling URL %s\" % url)\n",
    "                        \n",
    "                        headers_websites[\"Referer\"] = url\n",
    "                        \n",
    "                        try:\n",
    "                            r = requests.get(url, headers=headers_websites)\n",
    "                            r.encoding = r.apparent_encoding\n",
    "                        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.UnicodeDecodeError):\n",
    "                            try:\n",
    "                                r = requests.get(url, headers=headers_websites)\n",
    "                                r.encoding = r.apparent_encoding\n",
    "                            except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.UnicodeDecodeError):\n",
    "                                pass\n",
    "\n",
    "                                \n",
    "                            \n",
    "                        try:\n",
    "                            new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-zA-Z]+\", r.text if r.text else '', re.I))\n",
    "                            new_emails_updated = set([new_email for new_email in list(new_emails) if new_email.upper().find('.JPG') < 0 and new_email.upper().find('.JPEG') < 0 and new_email.upper().find('.PNG') < 0 and new_email.upper().find('.WEBP') < 0 and new_email.upper().find('.MP4') < 0])\n",
    "                            self.emails.update(new_emails_updated)\n",
    "                        except UnboundLocalError:\n",
    "                            pass\n",
    "                     \n",
    "\n",
    "                        soup = BeautifulSoup(r.text)\n",
    "                        current_urls = set()\n",
    "                        new_emails_set = set() \n",
    "\n",
    "                     \n",
    "                        for anchor in soup.find_all(\"a\"):\n",
    "\n",
    "                            if \"href\" in anchor.attrs:\n",
    "                                link = anchor.attrs[\"href\"]\n",
    "                            else:\n",
    "                                link = ''\n",
    "\n",
    "                            if (link.upper().find('.JPG') < 0 and link.upper().find('.JPEG') < 0 and link.upper().find('.PNG') < 0 and link.upper().find('.WEBP') < 0 and link.upper().find('.MP4') < 0) and link.find('javascript:') < 0:\n",
    "\n",
    "                                if link.startswith('mailto:'):\n",
    "                                    email = link.split(':')[1]\n",
    "                                    self.emails.add(email)\n",
    "                                else:\n",
    "                                    print(link)\n",
    "                                    if link.startswith('/') and not link.startswith('#'):\n",
    "                                        url = path + link\n",
    "                                    elif link.startswith(self.original_url) and link.upper().find('#') < 0:\n",
    "                                        url = link\n",
    "                                    \n",
    "                                    current_urls.add(url)\n",
    "                                    print(url)\n",
    "\n",
    "                        current_urls_list = list(current_urls)\n",
    "                        res = [i for i in current_urls_list if 'CONTACT' in i.upper()]\n",
    "                        print(current_urls_list)\n",
    "                        print(res)\n",
    "                        if len(res) > 0:\n",
    "                            for res_url in res:\n",
    "                                self.scrap_emails_from_url(res_url)\n",
    "                        for cur_url in current_urls_list:\n",
    "                            self.scrap_emails_from_url(cur_url)\n",
    "                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6878b6-0cb9-47fb-a0bd-619e30be3246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "email_scrapper = main_email_scrapper('https://archiconcept.fr/')\n",
    "emails = email_scrapper.finished()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f26080-d0e2-4316-a81c-6aa39d5fc056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143ba60-bd5e-4632-8093-f41bef5736ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
