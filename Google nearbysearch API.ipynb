{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfb37b-b4fa-4fe8-8e42-8a2ca931995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2 pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5d940f-3207-48ba-8d35-493a00e863c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3155/2221019325.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  input_data = pd.read_sql_query(sql_select_all, conn)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "languageCode = 'fr'\n",
    "t_host = \"DRAK\" # either \"localhost\", a domain name, or an IP address.\n",
    "t_port = \"5435\" # default postgres port\n",
    "t_dbname = \"postgres\"\n",
    "t_user = \"postgres\"\n",
    "t_pw = \"postgres\"\n",
    "\n",
    "input_data = None\n",
    "\n",
    "with psycopg2.connect(\"host='{}' port={} dbname={} user={} password={}\".format(t_host, t_port, t_dbname, t_user, t_pw)) as conn:\n",
    "    \n",
    "    sql_select_all = '''SELECT sub.id,  sub.geom_x,  sub.geom_y, sub.geom, sub.siren, sub.siret, sub.statutdiffusionetablissement, sub.adress, sub.adress_without_postal_code,\n",
    "                            sub.libellevoieetablissement, sub.libellecommuneetablissement, \n",
    "                            COALESCE(sub.enseigne1etablissement, sub.enseigne2etablissement, sub.enseigne3etablissement) as enseigneetablissement,\n",
    "                            COALESCE(sub.denominationusuelleetablissement, sub.enseigne1etablissement, sub.enseigne2etablissement, sub.enseigne3etablissement) as denominationusuelleetablissement \n",
    "                            FROM (SELECT id, geom, siren, siret, statutdiffusionetablissement, \n",
    "                                UPPER(CONCAT_WS(' ', NULLIF(numerovoieetablissement::text, '[ND]'), NULLIF(typevoieetablissement::text, '[ND]'), NULLIF(libellevoieetablissement::text, '[ND]'), NULLIF(codepostaletablissement::text, '[ND]'), NULLIF(libellecommuneetablissement::text, '[ND]'))) as adress,\n",
    "                                UPPER(CONCAT_WS(' ', NULLIF(numerovoieetablissement::text, '[ND]'), NULLIF(typevoieetablissement::text, '[ND]'), NULLIF(libellevoieetablissement::text, '[ND]'), NULLIF(libellecommuneetablissement::text, '[ND]'))) as adress_without_postal_code,\n",
    "                                NULLIF(UPPER(libellevoieetablissement), '[ND]') as libellevoieetablissement, \n",
    "                                NULLIF(UPPER(libellecommuneetablissement), '[ND]') as libellecommuneetablissement, \n",
    "                                NULLIF(denominationusuelleetablissement, '[ND]') as denominationusuelleetablissement,\n",
    "                                NULLIF(enseigne1etablissement, '[ND]') as enseigne1etablissement,\n",
    "                                NULLIF(enseigne2etablissement, '[ND]') as enseigne2etablissement,\n",
    "                                NULLIF(enseigne3etablissement, '[ND]') as enseigne3etablissement,\n",
    "                                ST_X(ST_GeomFromEWKT(geom)) as geom_x,\n",
    "                                ST_Y(ST_GeomFromEWKT(geom)) as geom_y\n",
    "                               FROM siren.siren_archis order by id ASC) sub;'''\n",
    "    input_data = pd.read_sql_query(sql_select_all, conn)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ead8a2-8ce4-4e73-91af-1274b0f9b49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "headers_websites = {\"Referer\":None,\n",
    "                    \"Sec-Ch-Ua\":\"'\\\"Not_A Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"120\\\", \\\"Google Chrome\\\";v=\\\"120\\\"'\",\n",
    "                    \"Sec-Ch-Ua-Mobile\":\"?0\",\n",
    "                    \"Sec-Ch-Ua-Platform\": \"Windows\",\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "                   }\n",
    "\n",
    "\n",
    "class main_email_scrapper:\n",
    "    \n",
    "    def __init__(self, original_url):\n",
    "        self.root_original_url = original_url\n",
    "        self.original_url = original_url\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.end_time =  self.start_time + datetime.timedelta(seconds=5)\n",
    "        # to save scraped urls\n",
    "        self.scraped = set()\n",
    "        self.steps = 0\n",
    "        # to save fetched emails\n",
    "        self.emails = set() \n",
    "        \n",
    "        self.scrap_emails_from_url(original_url)\n",
    "        \n",
    "        \n",
    "    def finished(self):\n",
    "        return list(self.emails)\n",
    "          \n",
    "    def scrap_emails_from_url(self, url):\n",
    "        if url.find('///') < 0 and url.find('tel:') < 0:\n",
    "            if self.steps < 2:\n",
    "                if datetime.datetime.now() >= self.end_time and len(list(self.emails)) < 1:\n",
    "                    self.end_time =  self.end_time + datetime.timedelta(seconds=5)\n",
    "                    self.steps += 1\n",
    "                if datetime.datetime.now() <= self.end_time:\n",
    "\n",
    "\n",
    "                    if url not in self.scraped:\n",
    "                        self.scraped.add(url)\n",
    "\n",
    "                        parts = urlsplit(url)\n",
    "\n",
    "                        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "\n",
    "                        if '/' in parts.path:\n",
    "                            path = url[:url.rfind('/')+1]\n",
    "                        else:\n",
    "                            path = url\n",
    "\n",
    "                        print(path)\n",
    "\n",
    "                        print(\"Crawling URL %s\" % url)\n",
    "                        \n",
    "                        headers_websites[\"Referer\"] = url\n",
    "                        www_added = False\n",
    "                        try_https = False\n",
    "                        \n",
    "                        try:\n",
    "                            if url.upper().find('WWW.') < 0 and not www_added:\n",
    "                                url = url.replace('//','//www.')\n",
    "                                self.original_url = self.root_original_url.replace('//','//www.')\n",
    "                                www_added = True\n",
    "                            else:\n",
    "                                self.original_url = self.root_original_url\n",
    "                            if url.upper().find('HTTP//.') >= 0 and not try_https:\n",
    "                                url.replace('http//','https//')\n",
    "                                self.original_url = self.root_original_url.replace('http//','https//')\n",
    "                                try_https = True\n",
    "                            else:\n",
    "                                self.original_url = self.original_url\n",
    "                                    \n",
    "                            r = requests.get(url, headers=headers_websites)\n",
    "                            r.encoding = r.apparent_encoding\n",
    "                        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, UnicodeDecodeError):\n",
    "                            try:\n",
    "                                if url.upper().find('WWW.') < 0 and not www_added:\n",
    "                                    url = url.replace('//','//www.')\n",
    "                                    self.original_url = self.root_original_url.replace('//','//www.')\n",
    "                                    www_added = True\n",
    "                                else:\n",
    "                                    self.original_url = self.root_original_url\n",
    "                                if url.upper().find('HTTP//.') >= 0 and not try_https:\n",
    "                                    url.replace('http//','https//')\n",
    "                                    self.original_url = self.root_original_url.replace('http//','https//')\n",
    "                                    try_https = True\n",
    "                                else:\n",
    "                                    self.original_url = self.original_url\n",
    "                                    \n",
    "                                r = requests.get(url, headers=headers_websites)\n",
    "                                r.encoding = r.apparent_encoding\n",
    "                            except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, UnicodeDecodeError):\n",
    "                                pass\n",
    "\n",
    "                                \n",
    "                            \n",
    "                        try:\n",
    "                            new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-zA-Z]+\", r.text if r.text else '', re.I))\n",
    "                            new_emails_updated = set([new_email for new_email in list(new_emails) if new_email.upper().find('.JPG') < 0 and new_email.upper().find('.JPEG') < 0 and new_email.upper().find('.PNG') < 0 and new_email.upper().find('.WEBP') < 0 and new_email.upper().find('.MP4') < 0])\n",
    "                            self.emails.update(new_emails_updated)\n",
    "                        except UnboundLocalError:\n",
    "                            pass\n",
    "                        \n",
    "                        soup = None\n",
    "                        current_urls = set()\n",
    "                        new_emails_set = set() \n",
    "                        \n",
    "                        try:\n",
    "                            soup = BeautifulSoup(r.text)\n",
    "                        except UnboundLocalError:\n",
    "                            soup = None\n",
    "                            \n",
    "                        if soup != None:\n",
    "                            for anchor in soup.find_all(\"a\"):\n",
    "\n",
    "                                if \"href\" in anchor.attrs:\n",
    "                                    link = anchor.attrs[\"href\"]\n",
    "                                else:\n",
    "                                    link = ''\n",
    "\n",
    "                                if (link.upper().find('.JPG') < 0 and link.upper().find('.JPEG') < 0 and link.upper().find('.PNG') < 0 and link.upper().find('.WEBP') < 0 and link.upper().find('.MP4') < 0) and link.find('javascript:') < 0:\n",
    "\n",
    "                                    if link.startswith('mailto:'):\n",
    "                                        email = link.split(':')[1]\n",
    "                                        self.emails.add(email)\n",
    "                                    else:\n",
    "                                        if link != self.original_url:\n",
    "                                            if link.startswith('/') and not link.startswith('#'):\n",
    "                                                url = path + link\n",
    "                                            elif link.startswith(self.original_url) and link.upper().find('#') < 0:\n",
    "                                                url = link\n",
    "\n",
    "                                            current_urls.add(url)\n",
    "                                  \n",
    "                            if len(list(current_urls)) > 0:\n",
    "                                current_urls_list = list(current_urls)\n",
    "                                res = [i for i in current_urls_list if 'CONTACT' in i.upper()]\n",
    "\n",
    "                                if len(res) > 0:\n",
    "                                    for res_url in res:\n",
    "                                        self.scrap_emails_from_url(res_url)\n",
    "                                for cur_url in current_urls_list:\n",
    "                                    self.scrap_emails_from_url(cur_url)\n",
    "                                            \n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79771861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "headers_google_api = {\"Content-type\": \"application/json\", \"X-Goog-Api-Key\": \"YOUR_GOOGLE_API_TOKEN\", \"X-Goog-FieldMask\": \"*\"}\n",
    "\n",
    "\n",
    "\n",
    "key = 'YOUR_GOOGLE_API_TOKEN'\n",
    "\n",
    "for index, data in input_data.iterrows():\n",
    "    input_name = None \n",
    "    mail_scrapper = None \n",
    "    all_data_results = []\n",
    "    \n",
    "    if data['enseigneetablissement'] is not None:\n",
    "        input_name = data['enseigneetablissement']\n",
    "    elif data['denominationusuelleetablissement'] is not None:\n",
    "        input_name = data['denominationusuelleetablissement']\n",
    "    else:\n",
    "        input_name = 'architecte'\n",
    "    \n",
    "    input_name = input_name.replace(' ', '%2C') + '%2C' + data[\"adress\"].replace(' ', '%2C')\n",
    "        \n",
    "    url = '''https://maps.googleapis.com/maps/api/place/nearbysearch/json?keyword={name}&location={coordinates}&radius={radius}&key={key}'''.format(name=input_name,\n",
    "                                                                                                                                                    coordinates=str(data['geom_y']) + '%2C' + str(data['geom_x']),\n",
    "                                                                                                                                                    radius=500,\n",
    "                                                                                                                                                    key=key\n",
    "                                                                                                                                                    )\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    \n",
    "    if r.status_code == 200 and r.text != '' and r.json() and str(r.json()) != '{}':\n",
    "        request_results = r.json()\n",
    "        if request_results[\"status\"] != \"ZERO_RESULTS\":\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            data_adress_matching_array = data['adress_without_postal_code'].replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip().split()\n",
    "\n",
    "\n",
    "            for y, place_result in enumerate(request_results[\"results\"]):\n",
    "                data_result = {\"ranking\": {\"adress_indice\":0, \"same_adress\":False, \"name_in_base\": False, \"same_name\":False, \"result_name\":\"\"}, \"place_result\": place_result}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if 'vicinity' in place_result.keys():\n",
    "\n",
    "                    adress_place_result = place_result['vicinity'].replace(',','').replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip()\n",
    "\n",
    "  \n",
    "                    if all(x in adress_place_result for x in data_adress_matching_array) is True:\n",
    "                        data_result[\"ranking\"][\"same_adress\"] = True\n",
    "                    found = 0\n",
    "                    for string in data_adress_matching_array:\n",
    "                        if string in adress_place_result:\n",
    "                            found += 1\n",
    "                    data_result[\"ranking\"][\"adress_indice\"] = found\n",
    "\n",
    "\n",
    "                if 'name' in place_result.keys():\n",
    "\n",
    "                    data_result[\"ranking\"][\"result_name\"] = place_result['name'].strip()\n",
    "\n",
    "\n",
    "                    if data['enseigneetablissement'] is not None:           \n",
    "                        name_place_compare = data['enseigneetablissement'].replace(' ',' ').replace(',','').replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip()\n",
    "                        if name_place_compare == place_result['name'].replace(',','').replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip():\n",
    "                           data_result[\"ranking\"][\"same_name\"] = True\n",
    "                    else:\n",
    "                        data_result[\"ranking\"][\"name_in_base\"] = False\n",
    "\n",
    "                    if data_result[\"ranking\"][\"same_name\"] is False:    \n",
    "                        if data['denominationusuelleetablissement'] is not None:\n",
    "                            name_place_compare = data['denominationusuelleetablissement'].replace(' ',' ').replace(',','').replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip()\n",
    "                            if name_place_compare == place_result['name'].replace(',','').replace('é','e').replace('è','e').replace('â','a').replace('ô','o').upper().strip():\n",
    "                               data_result[\"ranking\"][\"same_name\"] = True\n",
    "                        else:\n",
    "                            data_result[\"ranking\"][\"name_in_base\"] = False\n",
    "\n",
    "\n",
    "            \n",
    "                all_data_results.append(data_result)\n",
    "                \n",
    "\n",
    "            if len(all_data_results) > 0:\n",
    "                row_data_results = sorted(all_data_results, key=lambda item: item[\"ranking\"][\"same_adress\"])\n",
    "                row_data_results = sorted(row_data_results, key=lambda item: item[\"ranking\"][\"adress_indice\"], reverse=True)\n",
    "                row_data_results = sorted(row_data_results, key=lambda item: item[\"ranking\"][\"same_name\"])\n",
    "            \n",
    "\n",
    "                google_place_id = row_data_results[0][\"place_result\"][\"place_id\"]\n",
    "\n",
    "                url_get_place_by_id = \"https://places.googleapis.com/v1/places/{place_id}\".format(place_id=google_place_id)\n",
    "\n",
    "                r_details = requests.get(url_get_place_by_id, headers=headers_google_api)\n",
    "                r_details_results = {}\n",
    "\n",
    "\n",
    "                if r_details.status_code == 200 and r_details.text != '' and r_details.json() and str(r_details.json()) != '{}':\n",
    "                    r_details_results = r_details.json()\n",
    "                    phone = ''\n",
    "                    websiteUri = ''\n",
    "                    photos_names = set()\n",
    "                    google_profile_url = ''\n",
    "                    emails = []\n",
    "\n",
    "                    if 'internationalPhoneNumber' in r_details_results.keys():\n",
    "                        phone = r_details_results['internationalPhoneNumber']\n",
    "                    elif 'nationalPhoneNumber' in r_details_results.keys():\n",
    "                        phone = r_details_results['nationalPhoneNumber']\n",
    "\n",
    "                    if  \"websiteUri\" in r_details_results.keys():\n",
    "                        websiteUri = r_details_results['websiteUri']\n",
    "                        email_scrapper = main_email_scrapper(websiteUri)\n",
    "                        emails = email_scrapper.finished()\n",
    "\n",
    "                    if  \"displayName\" in r_details_results.keys():\n",
    "                        displayName = r_details_results['displayName']['text']\n",
    "\n",
    "\n",
    "\n",
    "                    if 'photos' in r_details_results.keys() and len(r_details_results['photos']) > 0:\n",
    "                        for photo in r_details_results['photos']:\n",
    "                            if 'authorAttributions' in photo.keys() and len(photo['authorAttributions']) > 0:\n",
    "                                photos_names.add((photo['authorAttributions'][0]['displayName'],photo['authorAttributions'][0]['uri']))    \n",
    "\n",
    "                    google_profile_url_array = [photo[1] for photo in photos_names if photo[0].upper().strip() == displayName.upper().strip()]\n",
    "                    if len(google_profile_url_array) > 1:\n",
    "                        google_profile_url = google_profile_url_array[0]\n",
    "\n",
    "\n",
    "\n",
    "                sql_upsert = '''INSERT INTO siren.google_place\n",
    "                                    (siren_archis_fk, \"result\", complete_return, place_details, google_place_id, website, phone, emails, google_profile_url)\n",
    "                                    VALUES ({siren_archis_fk}, '{result}'::json, '{complete_return}'::json, '{place_details}'::json, '{google_place_id}'::text, '{website}'::text, '{phone}'::text, '{emails}'::text[], '{google_profile_url}'::text)\n",
    "                                    ON CONFLICT (siren_archis_fk)\n",
    "                                    DO NOTHING;'''.format(siren_archis_fk=data['id'], \n",
    "                                                          result=json.dumps(row_data_results[0]).replace(\"'\", \"''\"), \n",
    "                                                          complete_return=json.dumps(row_data_results).replace(\"'\", \"''\"),\n",
    "                                                          place_details=json.dumps(r_details_results).replace(\"'\", \"''\"),\n",
    "                                                          google_place_id=google_place_id,\n",
    "                                                          website=websiteUri,\n",
    "                                                          phone=phone,\n",
    "                                                          emails='{' + ','.join(emails).replace(\"'\", \"''\") + '}' if len(emails) > 0 else '{NULL}',\n",
    "                                                          google_profile_url=google_profile_url\n",
    "                                                         )    \n",
    "\n",
    "\n",
    "                print(index, r.status_code, google_place_id)\n",
    "\n",
    "                with psycopg2.connect(\"host='{}' port={} dbname={} user={} password={}\".format(t_host, t_port, t_dbname, t_user, t_pw)) as conn:                       \n",
    "                    cur = conn.cursor()\n",
    "                    cur.execute(sql_upsert)\n",
    "                    conn.commit()\n",
    "                    cur.close()\n",
    "\n",
    "           \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
